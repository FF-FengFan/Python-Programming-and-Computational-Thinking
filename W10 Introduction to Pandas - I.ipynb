{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abea439",
   "metadata": {},
   "source": [
    "- Installation\n",
    "- Core components of pandas: Series and DataFrames (DF)\n",
    "- Reading in/Writing data to/from files\n",
    "- Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7fcb9",
   "metadata": {},
   "source": [
    "## What is pandas?\n",
    "\n",
    "1. Pandas is a data manipulation package in Python for tabular data. \n",
    "- Data in the form of rows and columns, also known as DataFrames. \n",
    "- Intuitively, you can think of a DataFrame as an Excel sheet. \n",
    "\n",
    "2. Pandas’ functionality\n",
    "- Data transformations, like sorting rows and taking subsets\n",
    "- Calculating summary statistics such as the mean\n",
    "- Reshaping DataFrames, and joining DataFrames together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6c3bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constructing DataFrame from a dictionary.\n",
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882608a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['col2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5de4ae",
   "metadata": {},
   "source": [
    "## What is pandas used for?\n",
    "- Import datasets from databases, spreadsheets, comma-separated values (CSV) files, and more.\n",
    "- Clean datasets, for example, by dealing with missing values.\n",
    "- Tidy datasets by reshaping their structure into a suitable format for analysis.\n",
    "- Aggregate data by calculating summary statistics such as the mean of columns, correlation between them, and more.\n",
    "- Visualize datasets and uncover insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071d80d",
   "metadata": {},
   "source": [
    "## Key benefits of the pandas package\n",
    "\n",
    "1. Made for Python: Python is the world's most popular language for machine learning and data science.\n",
    "\n",
    "2. Less verbose per unit operations: Code written in pandas is less verbose, requiring fewer lines of code to get the desired output. \n",
    "\n",
    "3. Intuitive view of data: pandas offers exceptionally intuitive data representation that facilitates easier data understanding and analysis.\n",
    "\n",
    "4. Extensive feature set: It supports an extensive set of operations from exploratory data analysis, dealing with missing values, calculating statistics, visualizing univariate and bivariate data, and much more.\n",
    "\n",
    "5. Works with large data: pandas handles large data sets with ease. It offers speed and efficiency while working with datasets of the order of millions of records and hundreds of columns, depending on the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d50c8",
   "metadata": {},
   "source": [
    "## Pandas Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9432dc",
   "metadata": {},
   "source": [
    "## Importing data in pandas\n",
    "\n",
    "- To begin working with pandas, import the pandas Python package. \n",
    "- When importing pandas, the most common alias for pandas is pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b952b",
   "metadata": {},
   "source": [
    "### 1. Importing CSV files\n",
    "\n",
    "1. Use read_csv() with the path to the CSV file to read a comma-separated values file\n",
    "2. This read operation loads the CSV file diabetes.csv to generate a pandas Dataframe object df. \n",
    "3. DataFrame.dtypes\n",
    "- This returns a Series with the data type of each column. The result’s index is the original DataFrame’s columns. Columns with mixed types are stored with the object dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pandas Dataframe object called df\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a5d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f18726",
   "metadata": {},
   "source": [
    "### 2. Importing text files\n",
    "\n",
    "1. The separator argument refers to the symbol used to separate rows in a DataFrame. \n",
    "\n",
    "2. Comma (sep = \",\"), whitespace(sep = \"\\s\"), tab (sep = \"\\t\"), and colon(sep = \":\") are the commonly used separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d26591",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes.txt\", sep=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33270f10",
   "metadata": {},
   "source": [
    "### 3. Importing Excel files (single sheet)\n",
    "1. Reading excel files (both XLS and XLSX) is as easy as the read_excel() function, using the file path as an input.\n",
    "\n",
    "2. You can also specify header which row becomes the DataFrame's header. \n",
    "- It has a default value of 0, which denotes the first row as headers or column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ff391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes.xlsx', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2d08f",
   "metadata": {},
   "source": [
    "### 4. Importing Excel files (multiple sheets)\n",
    "\n",
    "You just need to specify one additional argument, sheet_name, where you can either pass a string for the sheet name or an integer for the sheet position (note that Python uses 0-indexing, where the first sheet can be accessed with sheet_name = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0c3f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_multi.xlsx', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b70153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_multi.xlsx',sheet_name=1, header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fca92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_multi.xlsx',sheet_name='test_scores', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd52bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9c5fb",
   "metadata": {},
   "source": [
    "### 5. Importing JSON file\n",
    "you can use read_json() for JSON file types with the JSON file name as the argument.\n",
    "\n",
    "- JSON, also known as JavaScript Object Notation, is a data-interchange text-serialization format. JSON is easy to read and write. It is based on a subset of the JavaScript Programming Language but uses conventions from Python, and many other languages outside of Python.\n",
    "\n",
    "- JSON is mostly used to store unstructured data, and SQL databases have a tough time saving it. JSON makes the data accessible for the machines to read.\n",
    "\n",
    "- JSON is mainly built on two structures:\n",
    "\n",
    "    - A collection of key/value pairs. In Python, a key/value pair is referred to as a Dictionary, and a key is a unique attribute, whereas values are not.\n",
    "\n",
    "    - An ordered list of values. The ordered list can also sometimes be a list of lists. Lists in Python are a set of values which can be a string, integer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"diabetes.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3db1ee",
   "metadata": {},
   "source": [
    "If you want to learn more about importing data with pandas, check out this cheat sheet on importing various file types with Python.\n",
    "https://www.datacamp.com/cheat-sheet/importing-data-in-python-cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33992e6",
   "metadata": {},
   "source": [
    "## Outputting data in pandas\n",
    "\n",
    "- Just as pandas can import data from various file types, it also allows you to export data into various formats. \n",
    "- This happens especially when data is transformed using pandas and needs to be saved locally on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6e265",
   "metadata": {},
   "source": [
    "### 1. Outputting a DataFrame into a CSV file\n",
    "\n",
    "- A pandas DataFrame (here we are using df) is saved as a CSV file using the .to_csv() method. \n",
    "- The arguments include the filename with path and index – where index = True implies writing the DataFrame’s index.\n",
    "    - The index=False argument specifies that the index column should not be included in the output file.\n",
    "    - This is useful when the index is not meaningful or when it is already included as a separate column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code saves a pandas DataFrame df to a CSV file \n",
    "# named \"diabetes_out.csv\" in the current working directory.\n",
    "df.to_csv(\"diabetes_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9fbbe",
   "metadata": {},
   "source": [
    "### 2. Outputting a DataFrame into a JSON file\n",
    "Export DataFrame object into a JSON file by calling the .to_json() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"diabetes_out.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cc2a8",
   "metadata": {},
   "source": [
    "### 3. Outputting a DataFrame into a text file\n",
    "\n",
    "- As with writing DataFrames to CSV files, you can call .to_csv(). \n",
    "- The only differences are that the output file format is in .txt, and you need to specify a separator using the sep argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fce0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('diabetes_out.txt', header=df.columns, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70a8ed",
   "metadata": {},
   "source": [
    "- The header parameter is set to df.columns, which means that the column names of the DataFrame will be included as the first row in the output file.\n",
    "- The index parameter is set to None, which means that the row index of the DataFrame will not be included in the output file.\n",
    "- The sep parameter is set to a space character, which means that the values in each row will be separated by a space in the output file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fcf9b",
   "metadata": {},
   "source": [
    "### 4. Outputting a DataFrame into an Excel file\n",
    "\n",
    "Call .to_excel() from the DataFrame object to save it as a “.xls” or “.xlsx” file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"diabetes_out.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf2bba",
   "metadata": {},
   "source": [
    "## Viewing and understanding DataFrames using pandas \n",
    "\n",
    "- Row: observation\n",
    "- Column: variable\n",
    "- Every value within a column has the same data type, either text or numeric, but different columns can contain different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54783472",
   "metadata": {},
   "source": [
    "### 1. How to view data using .head() and .tail()\n",
    "- You can view the first few or last few rows of a DataFrame using the .head() or .tail() methods, respectively. \n",
    "- You can specify the number of rows through the n argument (the default value is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41407c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2588fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a11ed1",
   "metadata": {},
   "source": [
    "### 2. Understanding data using .describe()\n",
    "\n",
    "The DataFrame.describe() method prints the summary statistics of all numeric columns, such as count, mean, standard deviation, range, and quartiles of numeric columns.\n",
    "- The describe method computes some summary statistics for numerical columns, like mean and median.\n",
    "- \"count\": the number of non-missing values in each column\n",
    "- \"25%; 50%; 75%\": All percentiles should fall between 0 and 1. \n",
    "    - The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.\n",
    "    - The 50 percentile is the same as the median.\n",
    "- \"std\": Standard deviation of the observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df860c",
   "metadata": {},
   "source": [
    "- You can also modify the quartiles using the percentiles argument. \n",
    "- Here, for example, we’re looking at the 30%, 50%, and 70% percentiles of the numeric columns in DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.3, 0.5, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abadeeb",
   "metadata": {},
   "source": [
    "- You can also isolate specific data types in your summary output by using the include argument. \n",
    "- Here, for example, we’re only summarizing the columns with the integer data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[int])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3497fd",
   "metadata": {},
   "source": [
    "- Similarly, you might want to exclude certain data types using exclude argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76776247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(exclude=[int])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8eec69",
   "metadata": {},
   "source": [
    "- Often, practitioners find it easy to view such statistics by transposing them with the .T attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c91ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5249e6",
   "metadata": {},
   "source": [
    "For more on describing DataFrames, check out the following cheat sheet:\n",
    "\n",
    "https://www.datacamp.com/cheat-sheet/pandas-cheat-sheet-data-wrangling-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e46132",
   "metadata": {},
   "source": [
    "### 3. Understanding data using .info( )\n",
    "\n",
    "- float64: floating-point number format, usually occupying 64 bits in computer memory\n",
    "- int64: 64-bit integer\n",
    "\n",
    "The .info( ) method is a quick way to look at the data types, missing values, and data size of a DataFrame. \n",
    "- Here, we’re setting the show_counts argument to True, which gives a few over the total non-missing values in each column. \n",
    "- We’re also setting memory_usage to True, which shows the total memory usage of the DataFrame elements. \n",
    "- When verbose is set to True, it prints the full summary from .info(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576be2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info(show_counts=True, memory_usage=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f377338",
   "metadata": {},
   "source": [
    "### 4. Understanding your data using .shape\n",
    "\n",
    "- The number of rows and columns of a DataFrame can be identified using the .shape attribute of the DataFrame. \n",
    "- It returns a tuple (row, column) and can be indexed to get only rows, and only columns count as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # Get the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5385ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] # Get the number of rows only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[1] # Get the number of columns only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce15201",
   "metadata": {},
   "source": [
    "### 5. Get all columns and column names\n",
    "\n",
    "- Calling the .columns attribute of a DataFrame object returns the column names in the form of an Index object. \n",
    "- As a reminder, a pandas index is the address/label of the row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfd274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da272228",
   "metadata": {},
   "source": [
    "- It can be converted to a list using a list() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b4d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f85e5",
   "metadata": {},
   "source": [
    "- .index: the index atributes contains row numbers or row names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94421add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5c882",
   "metadata": {},
   "source": [
    "### 6. Checking for missing values in pandas with .isnull( )\n",
    "\n",
    "The sample DataFrame does not have any missing values. \n",
    "\n",
    "Let's introduce a few to make things interesting. \n",
    "- The .copy( ) method makes a copy of the original DataFrame. This is done to ensure that any changes to the copy don’t reflect in the original DataFrame. \n",
    "- Using .loc (to be discussed later), you can set rows two to five of the Pregnancies column to NaN values, which denote missing values.\n",
    "    - NaN (Not a Number): NaN represents missing or undefined data in Python. \n",
    "    - It is typically encountered while performing mathematical operations that result in an undefined or nonsensical value. NaN is a floating-point value represented by the float('nan') object in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5271da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.loc[2:5,'Pregnancies'] = None\n",
    "df2.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08b1dc",
   "metadata": {},
   "source": [
    "- You can check whether each element in a DataFrame is missing using the .isnull( ) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62400b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44120081",
   "metadata": {},
   "source": [
    "- Given it's often more useful to know how much missing data you have, you can combine .isnull() with .sum() to count the number of nulls in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5439bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e99ee0",
   "metadata": {},
   "source": [
    "- You can also do a double sum to get the total number of nulls in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f07fa4",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://www.datacamp.com/tutorial/pandas\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "\n",
    "https://www.datacamp.com/tutorial/importing-data-into-pandas\n",
    "\n",
    "https://app.datacamp.com/learn/courses/data-manipulation-with-pandas\n",
    "\n",
    "https://www.turing.com/kb/nan-values-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
