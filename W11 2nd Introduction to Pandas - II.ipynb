{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abea439",
   "metadata": {},
   "source": [
    "•\tInstallation\n",
    "\n",
    "•\tCore components of pandas: Series and DataFrames (DF)\n",
    "\n",
    "•\tReading in/Writing data to/from files\n",
    "\n",
    "•\tCleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7fcb9",
   "metadata": {},
   "source": [
    "## What is pandas?\n",
    "\n",
    "1. Pandas is a data manipulation package in Python for tabular data. \n",
    "- Data in the form of rows and columns, also known as DataFrames. \n",
    "- Intuitively, you can think of a DataFrame as an Excel sheet. \n",
    "\n",
    "2. Pandas’ functionality\n",
    "- Data transformations, like sorting rows and taking subsets\n",
    "- Calculating summary statistics such as the mean\n",
    "- Reshaping DataFrames, and joining DataFrames together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6c3bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constructing DataFrame from a dictionary.\n",
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882608a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['col2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5de4ae",
   "metadata": {},
   "source": [
    "## What is pandas used for?\n",
    "- Import datasets from databases, spreadsheets, comma-separated values (CSV) files, and more.\n",
    "- Clean datasets, for example, by dealing with missing values.\n",
    "- Tidy datasets by reshaping their structure into a suitable format for analysis.\n",
    "- Aggregate data by calculating summary statistics such as the mean of columns, correlation between them, and more.\n",
    "- Visualize datasets and uncover insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071d80d",
   "metadata": {},
   "source": [
    "## Key benefits of the pandas package\n",
    "\n",
    "1. Made for Python: Python is the world's most popular language for machine learning and data science.\n",
    "\n",
    "2. Less verbose per unit operations: Code written in pandas is less verbose, requiring fewer lines of code to get the desired output. \n",
    "\n",
    "3. Intuitive view of data: pandas offers exceptionally intuitive data representation that facilitates easier data understanding and analysis.\n",
    "\n",
    "4. Extensive feature set: It supports an extensive set of operations from exploratory data analysis, dealing with missing values, calculating statistics, visualizing univariate and bivariate data, and much more.\n",
    "\n",
    "5. Works with large data: pandas handles large data sets with ease. It offers speed and efficiency while working with datasets of the order of millions of records and hundreds of columns, depending on the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d50c8",
   "metadata": {},
   "source": [
    "## Pandas Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa180f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install pandas\n",
    "# pip install pandas\n",
    "# Or you can also try below\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9432dc",
   "metadata": {},
   "source": [
    "## Importing data in pandas\n",
    "\n",
    "- To begin working with pandas, import the pandas Python package. \n",
    "- When importing pandas, the most common alias for pandas is pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b952b",
   "metadata": {},
   "source": [
    "### 1. Importing CSV files\n",
    "\n",
    "1. Use read_csv() with the path to the CSV file to read a comma-separated values file\n",
    "2. This read operation loads the CSV file diabetes.csv to generate a pandas Dataframe object df. \n",
    "3. DataFrame.dtypes\n",
    "- This returns a Series with the data type of each column. The result’s index is the original DataFrame’s columns. Columns with mixed types are stored with the object dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/fanfeng/Downloads/diabetes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pandas Dataframe object called df\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a5d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f18726",
   "metadata": {},
   "source": [
    "### 2. Importing text files\n",
    "\n",
    "1. The separator argument refers to the symbol used to separate rows in a DataFrame. \n",
    "\n",
    "2. Comma (sep = \",\"), whitespace(sep = \"\\s\"), tab (sep = \"\\t\"), and colon(sep = \":\") are the commonly used separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d26591",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes.txt\", sep=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33270f10",
   "metadata": {},
   "source": [
    "### 3. Importing Excel files (single sheet)\n",
    "1. Reading excel files (both XLS and XLSX) is as easy as the read_excel() function, using the file path as an input.\n",
    "\n",
    "2. You can also specify header which row becomes the DataFrame's header. \n",
    "- It has a default value of 0, which denotes the first row as headers or column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ff391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes.xlsx', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2d08f",
   "metadata": {},
   "source": [
    "### 4. Importing Excel files (multiple sheets)\n",
    "\n",
    "You just need to specify one additional argument, sheet_name, where you can either pass a string for the sheet name or an integer for the sheet position (note that Python uses 0-indexing, where the first sheet can be accessed with sheet_name = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0c3f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_multi.xlsx', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b70153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_multi.xlsx',sheet_name=1, header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fca92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_multi.xlsx',sheet_name='test_scores', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd52bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9c5fb",
   "metadata": {},
   "source": [
    "### 5. Importing JSON file\n",
    "you can use read_json() for JSON file types with the JSON file name as the argument.\n",
    "\n",
    "- JSON, also known as JavaScript Object Notation, is a data-interchange text-serialization format. JSON is easy to read and write. It is based on a subset of the JavaScript Programming Language but uses conventions from Python, and many other languages outside of Python.\n",
    "\n",
    "- JSON is mostly used to store unstructured data, and SQL databases have a tough time saving it. JSON makes the data accessible for the machines to read.\n",
    "\n",
    "- JSON is mainly built on two structures:\n",
    "\n",
    "    - A collection of key/value pairs. In Python, a key/value pair is referred to as a Dictionary, and a key is a unique attribute, whereas values are not.\n",
    "\n",
    "    - An ordered list of values. The ordered list can also sometimes be a list of lists. Lists in Python are a set of values which can be a string, integer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"diabetes.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3db1ee",
   "metadata": {},
   "source": [
    "If you want to learn more about importing data with pandas, check out this cheat sheet on importing various file types with Python.\n",
    "https://www.datacamp.com/cheat-sheet/importing-data-in-python-cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33992e6",
   "metadata": {},
   "source": [
    "## Outputting data in pandas\n",
    "\n",
    "- Just as pandas can import data from various file types, it also allows you to export data into various formats. \n",
    "- This happens especially when data is transformed using pandas and needs to be saved locally on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6e265",
   "metadata": {},
   "source": [
    "### 1. Outputting a DataFrame into a CSV file\n",
    "\n",
    "- A pandas DataFrame (here we are using df) is saved as a CSV file using the .to_csv() method. \n",
    "- The arguments include the filename with path and index – where index = True implies writing the DataFrame’s index.\n",
    "    - The index=False argument specifies that the index column should not be included in the output file.\n",
    "    - This is useful when the index is not meaningful or when it is already included as a separate column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code saves a pandas DataFrame df to a CSV file \n",
    "# named \"diabetes_out.csv\" in the current working directory.\n",
    "df.to_csv(\"diabetes_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9fbbe",
   "metadata": {},
   "source": [
    "### 2. Outputting a DataFrame into a JSON file\n",
    "Export DataFrame object into a JSON file by calling the .to_json() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"diabetes_out.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cc2a8",
   "metadata": {},
   "source": [
    "### 3. Outputting a DataFrame into a text file\n",
    "\n",
    "- As with writing DataFrames to CSV files, you can call .to_csv(). \n",
    "- The only differences are that the output file format is in .txt, and you need to specify a separator using the sep argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fce0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('diabetes_out.txt', header=df.columns, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70a8ed",
   "metadata": {},
   "source": [
    "- The header parameter is set to df.columns, which means that the column names of the DataFrame will be included as the first row in the output file.\n",
    "- The index parameter is set to None, which means that the row index of the DataFrame will not be included in the output file.\n",
    "- The sep parameter is set to a space character, which means that the values in each row will be separated by a space in the output file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fcf9b",
   "metadata": {},
   "source": [
    "### 4. Outputting a DataFrame into an Excel file\n",
    "\n",
    "Call .to_excel() from the DataFrame object to save it as a “.xls” or “.xlsx” file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"diabetes_out.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf2bba",
   "metadata": {},
   "source": [
    "## Viewing and understanding DataFrames using pandas \n",
    "\n",
    "- Row: observation\n",
    "- Column: variable\n",
    "- Every value within a column has the same data type, either text or numeric, but different columns can contain different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54783472",
   "metadata": {},
   "source": [
    "### 1. How to view data using .head() and .tail()\n",
    "- You can view the first few or last few rows of a DataFrame using the .head() or .tail() methods, respectively. \n",
    "- You can specify the number of rows through the n argument (the default value is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41407c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2588fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a11ed1",
   "metadata": {},
   "source": [
    "### 2. Understanding data using .describe()\n",
    "\n",
    "The DataFrame.describe() method prints the summary statistics of all numeric columns, such as count, mean, standard deviation, range, and quartiles of numeric columns.\n",
    "- The describe method computes some summary statistics for numerical columns, like mean and median.\n",
    "- \"count\": the number of non-missing values in each column\n",
    "- \"25%; 50%; 75%\": All percentiles should fall between 0 and 1. \n",
    "    - The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.\n",
    "    - The 50 percentile is the same as the median.\n",
    "- \"std\": Standard deviation of the observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df860c",
   "metadata": {},
   "source": [
    "- You can also modify the quartiles using the percentiles argument. \n",
    "- Here, for example, we’re looking at the 30%, 50%, and 70% percentiles of the numeric columns in DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.3, 0.6, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abadeeb",
   "metadata": {},
   "source": [
    "- You can also isolate specific data types in your summary output by using the include argument. \n",
    "- Here, for example, we’re only summarizing the columns with the integer data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[int])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3497fd",
   "metadata": {},
   "source": [
    "- Similarly, you might want to exclude certain data types using exclude argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76776247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(exclude=[int])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8eec69",
   "metadata": {},
   "source": [
    "- Often, practitioners find it easy to view such statistics by transposing them with the .T attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c91ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5249e6",
   "metadata": {},
   "source": [
    "For more on describing DataFrames, check out the following cheat sheet:\n",
    "\n",
    "https://www.datacamp.com/cheat-sheet/pandas-cheat-sheet-data-wrangling-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e46132",
   "metadata": {},
   "source": [
    "### 3. Understanding data using .info( )\n",
    "\n",
    "The .info( ) method is a quick way to look at the data types, missing values, and data size of a DataFrame. \n",
    "- Here, we’re setting the show_counts argument to True, which gives a few over the total non-missing values in each column. \n",
    "- We’re also setting memory_usage to True, which shows the total memory usage of the DataFrame elements. \n",
    "- When verbose is set to True, it prints the full summary from .info(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576be2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info(show_counts=True, memory_usage=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f377338",
   "metadata": {},
   "source": [
    "### 4. Understanding your data using .shape\n",
    "\n",
    "- The number of rows and columns of a DataFrame can be identified using the .shape attribute of the DataFrame. \n",
    "- It returns a tuple (row, column) and can be indexed to get only rows, and only columns count as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # Get the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5385ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] # Get the number of rows only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[1] # Get the number of columns only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce15201",
   "metadata": {},
   "source": [
    "### 5. Get all columns and column names\n",
    "\n",
    "- Calling the .columns attribute of a DataFrame object returns the column names in the form of an Index object. \n",
    "- As a reminder, a pandas index is the address/label of the row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfd274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da272228",
   "metadata": {},
   "source": [
    "- It can be converted to a list using a list() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b4d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f85e5",
   "metadata": {},
   "source": [
    "- .index: the index atributes contains row numbers or row names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94421add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5c882",
   "metadata": {},
   "source": [
    "### 6. Checking for missing values in pandas with .isnull( )\n",
    "\n",
    "The sample DataFrame does not have any missing values. \n",
    "\n",
    "Let's introduce a few to make things interesting. \n",
    "- The .copy( ) method makes a copy of the original DataFrame. This is done to ensure that any changes to the copy don’t reflect in the original DataFrame. \n",
    "- Using .loc (to be discussed later), you can set rows two to five of the Pregnancies column to NaN values, which denote missing values.\n",
    "    - NaN (Not a Number): NaN represents missing or undefined data in Python. \n",
    "    - It is typically encountered while performing mathematical operations that result in an undefined or nonsensical value. NaN is a floating-point value represented by the float('nan') object in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5271da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.loc[2:5,'Pregnancies'] = None\n",
    "df2.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08b1dc",
   "metadata": {},
   "source": [
    "- You can check whether each element in a DataFrame is missing using the .isnull( ) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62400b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.isnull().head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44120081",
   "metadata": {},
   "source": [
    "- Given it's often more useful to know how much missing data you have, you can combine .isnull() with .sum() to count the number of nulls in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5439bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e99ee0",
   "metadata": {},
   "source": [
    "- You can also do a double sum to get the total number of nulls in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6846a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3af233",
   "metadata": {},
   "source": [
    "## Slicing and Extracting Data in pandas\n",
    "\n",
    "The pandas package offers several ways to subset, filter, and isolate data in your DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549d7a0",
   "metadata": {},
   "source": [
    "### 1. Isolating one column using [ ]\n",
    "\n",
    "- You can isolate a single column using a square bracket [ ] with a column name in it. \n",
    "- The output is a pandas Series object. \n",
    "- A pandas Series is a one-dimensional array containing data of any type, including integer, float, string, boolean, python objects, etc. \n",
    "    - Array: a special varilable, which can hold more than one value at a time. You can access the values by referring to an index number.\n",
    "    - Main different between list and array: https://www.geeksforgeeks.org/difference-between-list-and-array-in-python/\n",
    "        - List can consist of elements belonging to different data types\n",
    "        - Array only consists of elements belonging to the same data type\n",
    "    - Main different between list and series: https://www.geeksforgeeks.org/creating-a-pandas-series-from-lists/?ref=header_search\n",
    "        - List can consist of elements belonging to different data types\n",
    "        - Series will always contain data of the same type\n",
    "- A DataFrame is comprised of many series that act as columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79089d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b349b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b5f09",
   "metadata": {},
   "source": [
    "### 2. Isolating two or more columns using [[ ]] \n",
    "- You can also provide a list of column names inside the square brackets to fetch more than one column. \n",
    "- Here, square brackets are used in two different ways. \n",
    "    - We use the outer square brackets to indicate a subset of a DataFrame, and the inner square brackets to create a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Pregnancies', 'Outcome']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4841e8",
   "metadata": {},
   "source": [
    "### 3. Isolating one row using [ ] \n",
    "- A single row can be fetched by passing in a boolean series with one True value. \n",
    "- In the example below, the second row with index = 1 is returned. \n",
    "    - Here, .index returns the row labels of the DataFrame, and the comparison turns that into a Boolean one-dimensional array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225291c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.index==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cf027",
   "metadata": {},
   "source": [
    "### 4. Isolating two or more rows using [ ] \n",
    "Similarly, two or more rows can be returned using the .isin() method instead of a == operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360407a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index.isin(range(2,10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73bf93",
   "metadata": {},
   "source": [
    "### 5. Using .loc[] and .iloc[] to fetch rows\n",
    "You can fetch specific rows by labels or conditions using .loc[] and .iloc[] (\"location\" and \"integer location\"). \n",
    "- .loc[] uses a label to point to a row, column or cell\n",
    "    - we have to pass the name of the row or column which we want to select.\n",
    "    - .loc[] method includes the last element of the range passed in it.\n",
    "    - .loc() can accept the boolean data\n",
    "- .iloc() function is an indexed-based selecting method which means that we have to pass an integer index in the method to select a specific row/column. \n",
    "\n",
    "More comparisons between loc() and iloc() in Pandas: https://www.geeksforgeeks.org/difference-between-loc-and-iloc-in-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index = range(1,769)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d700716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The below example returns a pandas Series instead of a DataFrame. \n",
    "# The 1 represents the row index (label)\n",
    "df2.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93221227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 1 in .iloc[] is the row position (second row).\n",
    "df2.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112987f",
   "metadata": {},
   "source": [
    "### - You can also fetch multiple rows by providing a range in square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256b186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.loc[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a86b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805ccaf",
   "metadata": {},
   "source": [
    "### - You can also subset with .loc[] and .iloc[] by using a list instead of a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc0994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.loc[[100, 200, 300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41477e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[[100, 200, 300]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b2195",
   "metadata": {},
   "source": [
    "### - You can also select specific columns along with rows. \n",
    "This is where .iloc[] is different from .loc[] – it requires column location and not column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[100:110, ['Pregnancies', 'Glucose', 'BloodPressure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[100:110,['Pregnancies','Glucose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66270106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[100:110, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d5977",
   "metadata": {},
   "source": [
    "### - For faster workflows, you can pass in the starting index of a row as a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[760:, ['Pregnancies', 'Glucose', 'BloodPressure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[760:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529cc13",
   "metadata": {},
   "source": [
    "#### You can update/modify certain values by using the assignment operator ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e725ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign all rows, and compare if 'Age' ==1,\n",
    "# If it is True, I just reassign 'Age'=51\n",
    "df2.loc[df['Age']==50, ['Age']] = 51\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcf392",
   "metadata": {},
   "source": [
    "Q: pls return your output which includes rows label from 200:210, and only show two columns: 'Age' and 'Outcome'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First situation:\n",
    "df2 = df.copy()\n",
    "df2.loc[2:5,'Pregnancies'] = None\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9555401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: use .loc[]\n",
    "df2.loc[200:210, ['Age', 'Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038c1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method2: use .iloc[]\n",
    "df2.iloc[200:210, 7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second situation: redefine df2\n",
    "df2.index = range(1,769)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2ecf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method 1: use .loc[]\n",
    "df2.loc[200:210, ['Age', 'Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe09e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method 2: use .iloc[]\n",
    "df2.iloc[199:210,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65285213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method 2: use .iloc[]\n",
    "df2.iloc[199:210,7:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b09a66",
   "metadata": {},
   "source": [
    "### Conditional slicing (that fits certain conditions)\n",
    "\n",
    "pandas lets you filter data by conditions over row/column values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd55d8",
   "metadata": {},
   "source": [
    "#### Isolating rows based on a condition in pandas \n",
    "For example, the below code selects the row where Blood Pressure is exactly 122. \n",
    "- Here, we are isolating rows using the brackets [ ] as seen in previous sections. \n",
    "- However, instead of inputting row indices or column names, we are inputting a condition where the column BloodPressure is equal to 122. \n",
    "- We denote this condition using df.BloodPressure == 122."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BloodPressure == 122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc25df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.BloodPressure == 122]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754054ec",
   "metadata": {},
   "source": [
    "Q: Can you fetched all rows where Outcome is 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.Outcome == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748ffb4",
   "metadata": {},
   "source": [
    "- Here df.Outcome selects that column, \n",
    "- df.Outcome == 1 returns a Series of Boolean values determining which Outcomes are equal to 1, \n",
    "- then [] takes a subset of df where that Boolean Series is True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d3986",
   "metadata": {},
   "source": [
    "#### Isolating rows and columns based on a condition in pandas \n",
    "You can use a > operator to draw comparisons. \n",
    "\n",
    "The below code fetches Pregnancies, Glucose, and BloodPressure for all records with BloodPressure greater than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af8fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['BloodPressure'] > 100, ['Pregnancies','Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82193059",
   "metadata": {},
   "source": [
    "## Cleaning data using pandas \n",
    "Data cleaning is one of the most common tasks in data science. \n",
    "- pandas lets you preprocess data for any use, including but not limited to training machine learning and deep learning models. \n",
    "    - Let’s use the DataFrame df2 from earlier, having four missing values, to illustrate a few data cleaning use cases. \n",
    "    - As a reminder, here's how you can see how many missing values are in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11858871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.loc[2:5,'Pregnancies'] = None\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e31ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ac4dd",
   "metadata": {},
   "source": [
    "### Dealing with missing data technique #1: Dropping missing values\n",
    "\n",
    "One way to deal with missing data is to drop it. \n",
    "- This is particularly useful in cases where you have plenty of data and losing a small portion won’t impact the downstream analysis. \n",
    "- You can use a .dropna() method as shown below. \n",
    "- Here, we are saving the results from .dropna() into a DataFrame df3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ed0cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3 = df3.dropna()\n",
    "df3.shape # this is 4 rows less than df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff179e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d1e67",
   "metadata": {},
   "source": [
    "#### The axis argument lets you specify whether you are dropping rows, or columns, with missing values. \n",
    "\n",
    "- The default axis removes the rows containing NaNs. \n",
    "    - Use axis = 0 to remove the rows with one or more NaN values. \n",
    "    - Use axis = 1 to remove the columns with one or more NaN values. \n",
    "- Also, notice how we are using the argument inplace=True which lets you skip saving the output of .dropna() into a new DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d3923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3.dropna(inplace=True, axis=1)\n",
    "df3.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629262e",
   "metadata": {},
   "source": [
    "Q: Can you remove the rows with one or more NaN values from df3 using the axis argument?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f95551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddb77a",
   "metadata": {},
   "source": [
    "### Dealing with missing data technique #2: Replacing missing values\n",
    "\n",
    "Instead of dropping, replacing missing values with a summary statistic or a specific value (depending on the use case) maybe the best way to go. \n",
    "- For example, if there is one missing row from a temperature column denoting temperatures throughout the days of the week, replacing that missing value with the average temperature of that week may be more effective than dropping values completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "# Get the mean of Pregnancies\n",
    "mean_value = df3['Pregnancies'].mean()\n",
    "# Fill missing values using .fillna()\n",
    "df3 = df3.fillna(mean_value)\n",
    "df3.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24314e02",
   "metadata": {},
   "source": [
    "#### Dealing with Duplicate Data\n",
    "Let's add some duplicates to the original data to learn how to eliminate duplicates in a DataFrame. \n",
    "- Here, we are using the .concat() method to concatenate the rows of the df2 DataFrame to the df2 DataFrame, adding perfect duplicates of every row in df2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36610750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df2, df2])\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad15ad7",
   "metadata": {},
   "source": [
    "#### You can remove all duplicate rows (default) from the DataFrame using .drop_duplicates() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop_duplicates()\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4624b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8bf8c",
   "metadata": {},
   "source": [
    "### Renaming columns\n",
    "A common data cleaning task is renaming columns. \n",
    "- With the .rename() method, you can use columns as an argument to rename specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary for mapping old and new column names.\n",
    "df3 = df2.copy()\n",
    "df3.rename(columns = {'DiabetesPedigreeFunction':'DPF'}, inplace = True)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f469dee",
   "metadata": {},
   "source": [
    "#### You can also directly assign column names as a list to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age', 'Outcome', 'STF']\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637eebfc",
   "metadata": {},
   "source": [
    "For more on data cleaning, and for easier, more predictable data cleaning workflows, check out the following checklist, which provides you with a comprehensive set of common data cleaning tasks:\n",
    "https://www.datacamp.com/blog/infographic-data-cleaning-checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc27d1e",
   "metadata": {},
   "source": [
    "## Data analysis in pandas\n",
    "The main value proposition of pandas lies in its quick data analysis functionality. In this section, we'll focus on a set of analysis techniques you can use in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e345f19",
   "metadata": {},
   "source": [
    "### Summary operators (mean, mode, median)\n",
    "- you can get the mean of each column value using the .mean() method\n",
    "- A mode can be computed similarly using the .mode() method\n",
    "    - The mode is the number in a set of numbers that appears the most often.\n",
    "- the median of each column is computed with the .median() method\n",
    "    - It is the middle number in a sorted ascending or descending list of numbers \n",
    "    - Median can be more descriptive of that data set than the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2160bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the mean of columns in pandas\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cc93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the mode of columns in pandas\n",
    "df.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a872ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the median of columns in pandas\n",
    "df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee6165",
   "metadata": {},
   "source": [
    "### Create new columns based on existing columns \n",
    "\n",
    "pandas provides fast and efficient computation by combining two or more columns like scalar variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080a205",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Divides each value in the column Glucose with the corresponding value in the Insulin column \n",
    "# to compute a new column named Glucose_Insulin_Ratio.\n",
    "\n",
    "df2['Glucose_Insulin_Ratio'] = df2['Glucose']/df2['Insulin']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee474e",
   "metadata": {},
   "source": [
    "### Counting using .value_counts()\n",
    "\n",
    "Often times you'll work with categorical values, and you'll want to count the number of observations each category has in a column. \n",
    "- Category values can be counted using the .value_counts() methods. \n",
    "    - Here, for example, we are counting the number of observations where Outcome is diabetic (1) and the number of observations where the Outcome is non-diabetic (0).\n",
    "    - Adding the normalize argument returns proportions instead of absolute counts.\n",
    "    - Turn off automatic sorting of results using sort argument (True by default). The default sorting is based on the counts in descending order.\n",
    "    - You can also apply .value_counts() to a DataFrame object and specific columns within it instead of just a column. \n",
    "- Troubleshooting: https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .value_counts() in pandas\n",
    "df['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .value_counts() in pandas with normalization\n",
    "df['Outcome'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bfde68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using .value_counts() in pandas with sorting\n",
    "df['Outcome'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e176bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, for example, we are applying value_counts() on df with the subset argument, \n",
    "# which takes in a list of columns. \n",
    "df.value_counts(subset=['Pregnancies', 'Outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe08e7",
   "metadata": {},
   "source": [
    "### Aggregating data with .groupby() in pandas\n",
    "\n",
    "- pandas lets you aggregate values by grouping them by specific column values. \n",
    "- You can do that by combining the .groupby() method with a summary method of your choice.\n",
    "- .groupby() enables grouping by more than one column by passing a list of column names.\n",
    "- Any summary method can be used alongside .groupby(), including .min(), .max(), .mean(), .median(), .sum(), .mode(), and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6e424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the mean of each of the numeric columns grouped by Outcome.\n",
    "df.groupby('Outcome').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a667a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Pregnancies', 'Outcome']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f73112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the mean of each of the numeric columns grouped by Outcome.\n",
    "df.groupby('Outcome').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89537633",
   "metadata": {},
   "source": [
    "Q: Can you calculate the sum of each of the numeric columns grouped by Outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087e114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb2e1cf0",
   "metadata": {},
   "source": [
    "### Pivot Tables \n",
    "\n",
    "pandas also enables you to calculate summary statistics as pivot tables. \n",
    "\n",
    "This makes it easy to draw conclusions based on a combination of variables. \n",
    "\n",
    "- The below code picks the rows as unique values of Pregnancies, the column values are the unique values of Outcome, and the cells contain the average value of BMI in the corresponding group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data by pivoting with pandas\n",
    "pd.pivot_table(df, values=\"BMI\", index='Pregnancies', \n",
    "               columns=['Outcome'], aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8dc90",
   "metadata": {},
   "source": [
    "## Data visualization in pandas\n",
    "pandas provides convenience wrappers to Matplotlib plotting functions to make it easy to visualize your DataFrames.\n",
    "- Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.\n",
    "https://matplotlib.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f247719",
   "metadata": {},
   "source": [
    "### Line plots in pandas\n",
    "pandas enables you to chart out the relationships among variables using line plots.\n",
    "- figsize is a tuple of the width and height of the figure in inches\n",
    "- You can select the choice of colors by using the color argument.\n",
    "- All the columns of df can also be plotted on different scales and axes by using the subplots argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5601bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a line plot of BMI and Glucose versus the row index.\n",
    "df[['BMI', 'Glucose']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9982b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic line plot with pandas, with custom colors\n",
    "df[['BMI', 'Glucose']].plot.line(figsize=(20, 10), \n",
    "                                 color={\"BMI\": \"red\", \"Glucose\": \"blue\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplots for line plots with pandas\n",
    "df.plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888ed5c",
   "metadata": {},
   "source": [
    "### Bar plots in pandas\n",
    "For discrete columns, you can use a bar plot over the category counts to visualize their distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b430a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable Outcome with binary values is visualized below.\n",
    "df['Outcome'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44488f3",
   "metadata": {},
   "source": [
    "### Box plots in pandas\n",
    "The quartile distribution of continuous variables can be visualized using a boxplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec7535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The code below lets you create a boxplot with pandas.\n",
    "df.boxplot(column=['BMI'], by='Outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8814682",
   "metadata": {},
   "source": [
    "Q: When we choose to use a bar plot or a box plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f7688",
   "metadata": {},
   "source": [
    "1. Bar graphs (aka: bar charts or bar plots) present grouped data in a rectangular format. \n",
    "- They allow us to see and compare groups or categories based on the scores (often means) of another (often continuous) variable.\n",
    "2. Boxplots are an easy visual way to depict quartiles in your data. They are sometimes referred to as box-and-whisker-plots as they often have lines extending from the box data, which denote additional variablity outside of these quartiles. \n",
    "- Boxplots allow us a simple way to compare groups and view dispersion and spread in data. \n",
    "- They also help highlight outliers.\n",
    "\n",
    "Source: https://ademos.people.uic.edu/Chapter11.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f07fa4",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://www.datacamp.com/tutorial/pandas\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "\n",
    "https://www.datacamp.com/tutorial/importing-data-into-pandas\n",
    "\n",
    "https://app.datacamp.com/learn/courses/data-manipulation-with-pandas\n",
    "\n",
    "https://www.turing.com/kb/nan-values-in-python\n",
    "\n",
    "https://www.geeksforgeeks.org/difference-between-list-and-array-in-python/\n",
    "\n",
    "https://www.geeksforgeeks.org/creating-a-pandas-series-from-lists/?ref=header_search\n",
    "\n",
    "https://www.investopedia.com/terms/m/mode.asp\n",
    "\n",
    "https://www.investopedia.com/terms/m/median.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
